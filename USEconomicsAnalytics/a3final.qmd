---
title: "Assignment 3 - Data Analysis"
author: "Niranjan Krishnan Devaraj"
format: html
toc: true
editor: visual
embed-resources: true
---

## Data Analysis Project

#### Hey! In this project, we're going to be exploring the Housing Price Index (HPI) for United States, in the context of their Economic Indicators, S&P 500 Closing Values, Air Quality Index, and Geographical Locations. The data comes from several datasets, such as the Federal Housing Finance Agency, United State Environmental Protection Agency, International Monetary Fund, Yahoo Finance, and Google Maps API. This builds up on work done previously as part of Assignment 1, where HPI was analysed in the context of the US Economy.

#### This is the dataflow diagram of how the final dataset on which we're operating was formed. This process was done through several distinct processes on R, scripts for which are available on the associated GitHub Repo. This final dataset was then saved, and all operations thereafter was directly performed on this dataset.

```{mermaid}
graph TD;
    A[Combine AQI Data] --> B[combined_aqi_data.csv];
    B --> C[Combine with HPI_master.csv];
    C --> D[Combine with USIndicatorsEdited.csv];
    D --> E[Fetch GSPC Closing Prices];
    D --> F[Fetch Coordinates using Google Maps API];
    B --> F;
    C --> F;
    E --> G[Final Dataset];
    F --> G;
```

This diagram outlines our step-by-step approach to compiling a robust dataset for analysis. We start by bringing together air quality index data, then combine it with housing price indices and economic indicators for the US. Next, we incorporate data on GSPC closing prices and geographical coordinates. This comprehensive dataset enables us to explore correlations between air quality, housing prices, economic factors, and geographic locations, providing valuable insights for our analysis.

## Libraries Used

```{r}
#|echo: false
#|warnings: false
#|output: false

library(htmlwidgets)
library(corrplot)
library(tidyr)
library(dplyr)
library(MASS)
library(car)
library(ggplot2)
library(plotly)
library(leaflet)
library(knitr)
library(kableExtra)
```

## Data Read Operation

```{r}
# Read files into a dataframe
final_data <- read.csv("C:/Users/x/OneDrive/Documents/GitHub/analytics-assignment3/final_data1.csv")
final_data_clean <- read.csv("C:/Users/x/OneDrive/Documents/GitHub/analytics-assignment3/final_data_clean_VF.csv")
final_data_averaged <- read.csv("C:/Users/x/OneDrive/Documents/GitHub/analytics-assignment3/final_data_averaged.csv")
```

## Quantitative Analyses

### Correlation Analysis

```{r}
# Calculate correlation matrix
cor_matrix <- cor(final_data_clean[, sapply(final_data_clean, is.numeric)])

# Plot correlation matrix as a heatmap
corrplot(cor_matrix, method = "color", type = "upper", order = "hclust",          tl.col = "black", tl.srt = 45, tl.cex = 0.35)

# Find variables with strong correlations
strong_correlations <- colnames(cor_matrix)[rowSums(abs(cor_matrix) > 0.7) > 1]

# Print the variables with strong correlations  
print(strong_correlations)
```

When multiple variables in a dataset are strongly correlated, it creates a tricky situation for statistical analysis called multicollinearity. This essentially means that it's hard to tell how each individual variable contributes to the outcome we're interested in, whether it's air quality or economic factors like GDP. With such tangled relationships, the results of our analysis become less reliable. It's like trying to separate threads that are all tightly woven together. This can lead to inflated errors and confusing interpretations of our findings. To tackle multicollinearity, we need to carefully choose which variables to include in our analysis, transform the data if needed, or use specialized techniques to untangle the relationships between variables. Doing so ensures that our models are accurate and trustworthy, allowing us to draw meaningful conclusions from our data.

## Visualization of Trends over time

Before we proceed with modelling, let's observe the trends of the variables we're mapping in order to better understand it. This also serves as a way for us to understand the distribution of the data in each of these variables

```{r}
#visualizations

# Create an empty list to store plots
plot_list <- list()

# Aggregate the data by year
aggregated_data <- final_data_clean %>%
  group_by(Year) %>%
  summarize(across(where(is.numeric), ~ mean(., na.rm = TRUE)))

# Convert data to long format
aggregated_data_long <- aggregated_data %>%
  pivot_longer(cols = -Year, names_to = "Variable", values_to = "Value")

aggregated_data_long <- aggregated_data_long %>%
  mutate(Year = as.numeric(as.character(Year)))

# Loop over each numeric variable and generate a separate interactive plot
for (col in unique(aggregated_data_long$Variable)) {
  plot_data <- aggregated_data_long %>%
    filter(Variable == col)  # Select data for the current numeric variable
  
  # Create the plot
  plot <- plot_ly(plot_data, x = ~Year, y = ~Value, type = 'scatter', mode = 'lines') %>%
    layout(title = paste("Trend over Time for", col), xaxis = list(title = "Year"), yaxis = list(title = col))
  
  # Append the plot to the list
  plot_list[[col]] <- plot
}
```

```{r}
plot_list[[2]]
plot_list[[3]]
plot_list[[4]]
plot_list[[5]]
plot_list[[6]]
plot_list[[7]]
plot_list[[11]]
plot_list[[12]]
plot_list[[13]]
plot_list[[14]]
plot_list[[15]]

```

## Multiple Regression with Step Evaluation and Multicollinearity Tests

Using stepwise regression with the stepAIC (Akaike Information Criterion) method allows for automated variable selection by iteratively adding or removing predictors to optimize model fit while penalizing for model complexity, enhancing interpretability.

```{r}
#| output: false

# Remove CBSA, Year, and group columns
data_subset <- final_data_clean %>%
  select(-CBSA, -Year, -group)

# Perform stepwise regression
stepwise_model <- stepAIC(lm(index_nsa ~ ., data = data_subset), direction = "both")
```

## Multicollinearity

We're now going to removing multicollinear variables from the model trained by stepAIC. We've identified a list of variables, and through trial and error, we've been able to reduce the effect of multicollinearity in the model.

```{r}
# Step 2: Check for multicollinearity
vif_values <- vif(stepwise_model)
print(vif_values)

# Set a threshold for VIF values
threshold <- 10

# Identify variables with VIF above the threshold
high_collinearity_vars <- names(vif_values)[vif_values > threshold]

print(high_collinearity_vars)

# Remove variables with high collinearity from the model
final_model <- update(stepwise_model, . ~ . - 
                        Gross.domestic.product.per.capita..constant.prices - GSPC.Close - 
                        Gross.domestic.product.based.on.purchasing.power.parity..PPP..share.of.world.total )

# Check for multicollinearity again
vif_values <- vif(final_model)
print(vif_values)

# Set a threshold for VIF values
threshold <- 10

# Identify variables with VIF above the threshold
high_collinearity_vars <- names(vif_values)[vif_values > threshold]

print(high_collinearity_vars)

# Summary of the updated final model
summary(final_model)
```

This multiple regression model shows a strong overall fit with an Adjusted R-squared value of 0.6902, indicating that approximately 69.02% of the variance in the dependent variable (index_nsa) is explained by the independent variables included in the model.

Several predictors exhibit significant relationships with the dependent variable, including Days.with.AQI, Moderate.Days, Max.AQI, Median.AQI, Days.CO, Days.NO2, Days.Ozone, Inflation, Volume.of.imports.of.goods.and.services, Volume.of.exports.of.goods.and.services, Unemployment.rate, Current.account.balance, lon_numeric, and lat_numeric, as indicated by their low p-values (p \< 0.05).

However, some predictors such as Unhealthy.for.Sensitive.Groups.Days and Gross.domestic.product..constant.prices do not show statistically significant associations with the dependent variable, suggesting their limited contribution to the model's predictive power.

The relationships we've uncovered between various factors and our outcomes, like the Housing Price Index (HPI) or air quality index (AQI), provide us with some fascinating insights. For instance, when we look at metrics like the number of days with poor air quality or the severity of pollution, we get a clear picture of how it impacts housing prices and people's overall well-being. On the economic side, indicators like inflation rates, trade volumes, and unemployment rates give us clues about how economic conditions shape housing markets.

What's particularly interesting is that some factors, like days when the air quality is unhealthy for sensitive groups or GDP at constant prices, don't seem to have a big impact on our outcomes. This tells us that while they're important, there are other factors, like pollutant concentrations and broader economic indicators, that play a bigger role in influencing housing prices and air quality. These insights are crucial for policymakers and city planners as they work to address housing affordability and environmental concerns in our communities.

The residual standard error of 40.24 indicates the average deviation of observed values from the fitted values, providing a measure of the model's goodness of fit.

## Qualitative Analyses

#### Geospatial Data Visualization to Discover Patterns and Relationships

Geospatial data visualization is a powerful technique used to uncover patterns, trends, and relationships within spatially referenced data. By mapping data onto geographical coordinates, we are able to visualize complex spatial distributions, identify clusters, and understand spatial interactions between variables.

```{r}
#index_nsa

# Define a function to create Leaflet map with custom popup and heatmap
create_leaflet_map <- function(data) {
  pal <- colorQuantile(c("green", "red"), domain = data$index_nsa, n = 5)  # Define color palette
  
  leaflet(data) %>%
    addTiles() %>%
    addCircleMarkers(
      lng = ~lon_numeric,
      lat = ~lat_numeric,
      radius = 7,
      color = ~pal(index_nsa),  # Use color palette based on index_nsa values
      fillOpacity = 0.7,
      popup = paste(
        "<b>CBSA:</b> ", data$CBSA, "<br>",
        "<b>Year:</b> ", data$Year, "<br>",
        "<b>Good Days:</b> ", data$Good.Days, "<br>",
        "<b>Max AQI:</b> ", data$Max.AQI, "<br>",
        "<b>Index NSA:</b> ", data$index_nsa
      ),
      label = ~substr(data$CBSA, 1, 20), # Shorten CBSA name for label
      labelOptions = labelOptions(noHide = FALSE)
    ) %>%
    addLegend(
      position = "bottomright",
      pal = pal,
      values = ~index_nsa,
      title = "Index NSA"
    )
}

# Call the function for each group
maps <- list()

for (i in unique(final_data_averaged$group)) {
  data <- filter(final_data_averaged, group == i)
  map <- create_leaflet_map(data)
  title <- htmltools::h2(paste("Housing Price Index (Heatmap) across a Geographic Region - Group ", i),
                         style = "font-family: Arial; font-size: 18px; color: Black; text-align: center;")
  maps[[i]] <- prependContent(map, title)
}
```

```{r}
maps[[1]]
maps[[2]]
maps[[3]]
maps[[4]]
maps[[5]]
maps[[6]]
maps[[7]]
```

In our exploration of housing trends, we've uncovered a concerning pattern in the western part of the USA: steadily increasing housing costs. What's driving this phenomenon? Well, it's a mix of factors. Take a look at cities like San Francisco, Los Angeles, Seattle, and Denver. They're buzzing with economic opportunities, drawing people in with promises of thriving job markets and enviable lifestyles.

But with all this growth comes a downside: a surge in demand for housing that surpasses what's available. This mismatch between supply and demand has pushed housing prices through the roof, making affordable options scarce. And it's not just about people wanting to move in – regulations on zoning and land use, coupled with limited space for development, have tightened the squeeze even further.

Adding to the mix are speculative real estate ventures and foreign investments pouring into urban areas, driving prices up even more. It's a tough situation for folks, especially those with modest incomes, who find themselves grappling with the challenge of finding housing that fits their budget.\

```{r}
#aqi

# Define a function to create Leaflet map with custom popup and heatmap
create_leaflet_map2 <- function(data) {
  pal <- colorQuantile(c("green", "red"), domain = data$Max.AQI, n = 5)  # Define color palette
  
  leaflet(data) %>%
    addTiles() %>%
    addCircleMarkers(
      lng = ~lon_numeric,
      lat = ~lat_numeric,
      radius = 7,
      color = ~pal(index_nsa),  # Use color palette based on index_nsa values
      fillOpacity = 0.7,
      popup = paste(
        "<b>CBSA:</b> ", data$CBSA, "<br>",
        "<b>Year:</b> ", data$Year, "<br>",
        "<b>Good Days:</b> ", data$Good.Days, "<br>",
        "<b>Max AQI:</b> ", data$Max.AQI, "<br>",
        "<b>Index NSA:</b> ", data$index_nsa
      ),
      label = ~substr(data$CBSA, 1, 20), # Shorten CBSA name for label
      labelOptions = labelOptions(noHide = FALSE)
    ) %>%
    addLegend(
      position = "bottomright",
      pal = pal,
      values = ~Max.AQI,
      title = "Average Max AQI"
    )
}

# Call the function for each group
maps2 <- list()
for (i in unique(final_data_averaged$group)) {
  data <- filter(final_data_averaged, group == i)
  map <- create_leaflet_map2(data)
  title <- htmltools::h2(paste("Air Quality Index (Heatmap) across a Geographic region - Group ", i),
                         style = "font-family: Arial; font-size: 18px; color: black; text-align: center;")
  maps2[[i]] <- htmlwidgets::prependContent(map, title)
}
```

```{r}
maps2[[1]]
maps2[[2]]
maps2[[3]]
maps2[[4]]
maps2[[5]]
maps2[[6]]
maps2[[7]]
```

From 1990 to 2023, the air quality index (AQI) in the USA has shown a concerning trend. Initially, there were positive outcomes as environmental regulations and technological advancements led to improvements in AQI. However, challenges emerged with the continued growth of urbanization, industrialization, and population density, particularly along borders and in densely populated areas.

Increased vehicular traffic, industrial emissions, and energy consumption have contributed to higher levels of air pollution, gradually deteriorating the AQI. Geographical factors such as proximity to major transportation routes and industrial zones have exacerbated air quality issues.

As pollution levels intensified, especially in heavily populated areas, addressing air quality concerns became increasingly urgent. Comprehensive regulatory measures, technological innovations, and public awareness campaigns are needed to mitigate the impact of air pollution on public health and the environment.\

```{r}
# Define the list of variables to include in the subset
variables_to_include <- c(
  "GSPC.Close",
  "Gross.domestic.product..constant.prices",
  "Gross.domestic.product.per.capita..constant.prices",
  "Gross.domestic.product.per.capita..current.prices",
  "Gross.domestic.product.based.on.purchasing.power.parity..PPP..share.of.world.total",
  "Inflation..average.consumer.prices",
  "Volume.of.imports.of.goods.and.services",
  "Volume.of.exports.of.goods.and.services",
  "Unemployment.rate"
)

# Create the subset
subset_data <- final_data_averaged %>%
  select(Year,group, all_of(variables_to_include))

# Filter subset_data for the specified years
filtered_data <- subset_data %>%
  filter(Year %in% c(1991, 1996, 2001, 2006, 2011, 2016, 2021, 2023)) %>%
  distinct(Year, .keep_all = TRUE)

# Create a vector of years to highlight
highlight_years <- c(1991, 1996, 2001, 2006, 2011, 2016, 2021, 2023)

# Highlight the rows corresponding to the specified years
filtered_data$Year_highlight <- ifelse(filtered_data$Year %in% highlight_years, "background-color: #FFFF00", "")

# Display the formatted table
filtered_data %>%
  arrange(Year) %>%
  select(-Year_highlight) %>%
  kable(caption = "Filtered Data for Selected Years", align = "c") %>%
  kable_styling(full_width = FALSE) %>%
  row_spec(row = which(filtered_data$Year %in% highlight_years), background = "#FFFF00")

```

This table provides insights into various aspects of US society, offering a qualitative analysis of trends observed over the years.

Regarding stock market performance, the data shows a consistent upward trend in the S&P 500 index from 1996 to 2023, indicating overall growth and stability in the stock market.

Economic growth is reflected in the increasing trend of Gross Domestic Product (GDP) measures. Both GDP per capita at constant and current prices show steady growth over time, suggesting improvements in living standards and economic prosperity.

Inflation rates fluctuate over the years but generally remain within acceptable ranges, indicating stable economic conditions despite variations in consumer price levels.

Trade volume, represented by the volume of imports and exports, demonstrates fluctuations influenced by global economic conditions, trade policies, and exchange rates.

Lastly, the unemployment rate shows variations over time, influenced by economic growth, business cycles, and government policies, highlighting the dynamic nature of labor market conditions.

Overall, these metrics provide valuable insights into the economic landscape of the United States, offering a nuanced understanding of trends and patterns shaping various aspects of society.

In summary, this project comprehensively analyzed the United States' economic indicators, housing prices, and air quality trends. By employing advanced statistical techniques and geospatial visualization, it uncovered significant correlations, identified key trends, and provided valuable insights into the complex dynamics shaping societal and environmental factors. These findings have the potential to contribute to a deeper understanding of regional disparities and inform evidence-based strategies for addressing housing affordability in the United States.
